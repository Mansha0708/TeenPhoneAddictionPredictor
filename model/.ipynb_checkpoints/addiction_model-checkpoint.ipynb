{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d241286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"train_model.ipynb\n",
    "\n",
    "This notebook will preprocess the teen_phone_addiction_dataset.csv,\n",
    "engineer new features, and then train a RandomForestRegressor model.\n",
    "Finally, it will save the trained model as 'addiction_model.pkl'.\n",
    "\"\"\"\n",
    "\n",
    "# 1. Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder # For initial categorical encoding if needed, or for reference\n",
    "\n",
    "# --- Configuration ---\n",
    "# Adjust this path based on where your notebook is relative to the CSV file.\n",
    "# Assuming notebook is in 'teenaddiction/model/' and CSV is in 'teenaddiction/data/'\n",
    "DATASET_PATH = 'D:\\c++\\teenaddiction\\data\\teen_phone_addiction_dataset.csv'\n",
    "MODEL_OUTPUT_PATH = 'D:\\c++\\teenaddiction\\model\\addiction_model.pkl' # Model will be saved in the same directory as the notebook\n",
    "\n",
    "# --- 2. Load your dataset ---\n",
    "try:\n",
    "    df = pd.read_csv(DATASET_PATH)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    print(f\"Initial shape: {df.shape}\")\n",
    "    print(\"Initial columns:\", df.columns.tolist())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Dataset not found at {DATASET_PATH}.\")\n",
    "    print(\"Please ensure 'teen_phone_addiction_dataset.csv' is in the 'data/' directory.\")\n",
    "    # Exit or handle the error appropriately\n",
    "    exit()\n",
    "\n",
    "# --- 3. Data Cleaning and Preprocessing ---\n",
    "\n",
    "# Define columns to drop (identifiers and potentially redundant/leaky target-related columns)\n",
    "columns_to_drop_initial = [\"ID\", \"Name\", \"Location\", \"Depression_Class\", \"Depression_Level_Class\", \"Anxiety_Class\", \"Usage_Bin\", \"Usage_Group\"]\n",
    "# Filter to ensure we only try to drop columns that actually exist\n",
    "existing_columns_to_drop = [col for col in columns_to_drop_initial if col in df.columns]\n",
    "\n",
    "if existing_columns_to_drop:\n",
    "    df.drop(columns=existing_columns_to_drop, inplace=True)\n",
    "    print(f\"\\nDropped columns: {existing_columns_to_drop}\")\n",
    "else:\n",
    "    print(\"\\nNo specified initial columns to drop were found in the DataFrame (or already dropped).\")\n",
    "\n",
    "# Handle missing values (dropping rows for simplicity; consider imputation for larger datasets)\n",
    "print(\"\\nMissing values before dropping NaNs:\\n\", df.isnull().sum())\n",
    "df.dropna(inplace=True)\n",
    "print(\"\\nMissing values after dropping NaNs:\\n\", df.isnull().sum())\n",
    "\n",
    "# Encode categorical columns using One-Hot Encoding\n",
    "# These are the original categorical columns that need to be transformed\n",
    "categorical_cols_to_encode = ['Gender', 'School_Grade', 'Phone_Usage_Purpose']\n",
    "existing_categorical_cols = [col for col in categorical_cols_to_encode if col in df.columns]\n",
    "\n",
    "if existing_categorical_cols:\n",
    "    # Using pd.get_dummies for one-hot encoding\n",
    "    # drop_first=True to avoid multicollinearity\n",
    "    df = pd.get_dummies(df, columns=existing_categorical_cols, drop_first=True)\n",
    "    print(f\"\\nOne-hot encoded columns: {existing_categorical_cols}\")\n",
    "else:\n",
    "    print(\"\\nNo specified categorical columns to encode were found.\")\n",
    "\n",
    "# --- 4. Feature Engineering ---\n",
    "# Ensure the base columns for feature engineering exist in the DataFrame\n",
    "# Check for presence of all required base columns before engineering\n",
    "required_base_cols = [\n",
    "    'Daily_Usage_Hours', 'Screen_Time_Before_Bed', 'Time_on_Social_Media',\n",
    "    'Time_on_Education', 'Time_on_Gaming', 'Weekend_Usage_Hours',\n",
    "    'Phone_Checks_Per_Day', 'Apps_Used_Daily', 'Sleep_Hours'\n",
    "]\n",
    "\n",
    "if all(col in df.columns for col in required_base_cols):\n",
    "    df['Night_Usage'] = df['Daily_Usage_Hours'] - df['Screen_Time_Before_Bed']\n",
    "    # Add +1 to denominators to prevent division by zero for ratio features\n",
    "    df['Social_to_Edu_Ratio'] = (df['Time_on_Social_Media'] + 1) / (df['Time_on_Education'] + 1)\n",
    "    df['Gaming_to_Social_Ratio'] = (df['Time_on_Gaming'] + 1) / (df['Time_on_Social_Media'] + 1)\n",
    "    df['Weekend_Overuse'] = df['Weekend_Usage_Hours'] - df['Daily_Usage_Hours']\n",
    "    df['Phone_Obsessiveness'] = df['Phone_Checks_Per_Day'] / (df['Apps_Used_Daily'] + 1)\n",
    "    df['Sleep_Deficit'] = 8 - df['Sleep_Hours'] # Assuming 8 hours is ideal sleep\n",
    "    print(\"\\nEngineered new features.\")\n",
    "else:\n",
    "    print(\"\\nWarning: Some base columns for feature engineering are missing. Skipping feature engineering.\")\n",
    "    print(\"Missing base columns:\", [col for col in required_base_cols if col not in df.columns])\n",
    "\n",
    "\n",
    "# --- 5. Define features (X) and target (y) ---\n",
    "# The target variable is 'Addiction_Level'\n",
    "if 'Addiction_Level' in df.columns:\n",
    "    X = df.drop(columns=['Addiction_Level'])\n",
    "    y = df['Addiction_Level']\n",
    "    print(f\"\\nFeatures (X) shape: {X.shape}\")\n",
    "    print(f\"Target (y) shape: {y.shape}\")\n",
    "    print(\"\\nFeatures used for training (X.columns.tolist()):\")\n",
    "    print(X.columns.tolist()) # This list is crucial for your Streamlit app\n",
    "else:\n",
    "    print(\"Error: 'Addiction_Level' column not found in the DataFrame. Cannot define target.\")\n",
    "    exit()\n",
    "\n",
    "# --- 6. Train/test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"\\nTraining data shape: {X_train.shape}, Test data shape: {X_test.shape}\")\n",
    "\n",
    "# --- 7. Train model ---\n",
    "print(\"\\nTraining RandomForestRegressor model...\")\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42) # You can adjust n_estimators\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- 8. Evaluate model ---\n",
    "preds = model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, preds, squared=False) # RMSE\n",
    "r2 = r2_score(y_test, preds) # R-squared\n",
    "print(f\"\\nModel Evaluation:\")\n",
    "print(f\"  RMSE: {rmse:.2f}\")\n",
    "print(f\"  R-squared: {r2:.2f}\")\n",
    "\n",
    "# --- 9. Save model ---\n",
    "joblib.dump(model, MODEL_OUTPUT_PATH)\n",
    "print(f\"\\nâœ… Model saved as '{MODEL_OUTPUT_PATH}'\")\n",
    "\n",
    "print(\"\\n--- Training Script Finished ---\")\n",
    "print(\"Next steps:\")\n",
    "print(f\"1. The '{MODEL_OUTPUT_PATH}' file is now saved in your 'teenaddiction/model/' directory.\")\n",
    "print(f\"2. Ensure your Streamlit app ('app/app.py') uses the exact same feature list and order for prediction.\")\n",
    "print(\"   The feature list your model expects is printed above (X.columns.tolist()).\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
